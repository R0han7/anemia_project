{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Real Data Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from wand.image import Image\n",
    "\n",
    "# def convert_heic_to_jpg_wand(data_dir):\n",
    "#     # Walk through the directory\n",
    "#     for root, dirs, files in os.walk(data_dir):\n",
    "#         for file in files:\n",
    "#             # Check if the file is a HEIC file\n",
    "#             if file.lower().endswith('.heic'):\n",
    "#                 heic_file_path = os.path.join(root, file)\n",
    "                \n",
    "#                 # Print the HEIC file name\n",
    "#                 print(f\"Processing: {file}\")\n",
    "                \n",
    "#                 # Convert HEIC to JPG using Wand\n",
    "#                 try:\n",
    "#                     with Image(filename=heic_file_path) as img:\n",
    "#                         img.format = 'jpeg'\n",
    "#                         jpg_file_path = os.path.splitext(heic_file_path)[0] + '.jpg'\n",
    "#                         img.save(filename=jpg_file_path)\n",
    "\n",
    "#                     # Delete the original HEIC file after conversion\n",
    "#                     os.remove(heic_file_path)\n",
    "#                     print(f\"Converted and deleted: {file}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Failed to process {file}: {e}\")\n",
    "\n",
    "# # Define your dataset directory\n",
    "# dataset_dir = r'.\\real_data'\n",
    "\n",
    "# # Call the function to convert HEIC files to JPG and delete them\n",
    "# convert_heic_to_jpg_wand(dataset_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# List of possible image extensions\n",
    "image_formats = {'.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG', '.heic', '.HEIC'}\n",
    "\n",
    "def get_image_files(patient_path):\n",
    "    \"\"\" Get all image files in a patient's folder \"\"\"\n",
    "    image_files = []\n",
    "    for file in os.listdir(patient_path):\n",
    "        if any(file.lower().endswith(ext) for ext in image_formats):\n",
    "            image_files.append(os.path.join(patient_path, file))\n",
    "    return image_files\n",
    "\n",
    "class AnemiaDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Walk through the dataset directories (IRON DEFICIENCY ANEMIA and NON - IRON DEFICIENCY ANEMIA)\n",
    "        for condition in ['IRON DEFICIENCY ANEMIA', 'NON - IRON DEFICIENCY ANEMIA']:\n",
    "            condition_path = os.path.join(data_dir, condition)\n",
    "            if not os.path.exists(condition_path):\n",
    "                continue\n",
    "            \n",
    "            # Iterate through each patient folder\n",
    "            for patient_folder in os.listdir(condition_path):\n",
    "                patient_path = os.path.join(condition_path, patient_folder)\n",
    "                if os.path.isdir(patient_path):\n",
    "                    # Get all image files for the patient (regardless of their name)\n",
    "                    img_files = get_image_files(patient_path)\n",
    "                    \n",
    "                    # If any images are found, add them to the dataset\n",
    "                    for img_path in img_files:\n",
    "                        self.image_paths.append(img_path)\n",
    "                        # Set label: 1 for \"IRON DEFICIENCY ANEMIA\", 0 for \"NON - IRON DEFICIENCY ANEMIA\"\n",
    "                        self.labels.append(1 if condition == 'IRON DEFICIENCY ANEMIA' else 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load the image\n",
    "        image = Image.open(img_path).convert('RGB')  # Ensure it's RGB format\n",
    "        \n",
    "        # Apply any transformations (resize, normalize, etc.)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Define the transformations to apply to each image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Slightly larger for random crop\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.15, 0.15), scale=(0.8, 1.2)),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define the path to your dataset\n",
    "dataset_dir = r\".\\real_data\"\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = AnemiaDataset(data_dir=dataset_dir, transform=transform)\n",
    "\n",
    "# Print the total number of images found\n",
    "print(f\"Total number of images found: {len(dataset)}\")\n",
    "\n",
    "# Split the dataset into training and validation\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size = len(dataset) - train_size  # 20% for validation\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Example of how to access the data\n",
    "train_labels=[]\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape)  # Batch of images\n",
    "    train_labels.append(labels)      # Corresponding labels\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all training labels\n",
    "all_train_labels = []\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    all_train_labels.extend(labels.tolist())  # Convert tensor to list and extend the list\n",
    "\n",
    "# Convert the list of labels to a tensor\n",
    "# all_train_labels = torch.tensor(all_train_labels)\n",
    "\n",
    "# Print the collected training labels\n",
    "print(all_train_labels)\n",
    "print(len(all_train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the dataset directory\n",
    "dataset_dir = r\".\\real_data\"\n",
    "\n",
    "# List of possible image extensions\n",
    "image_formats = {'.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG', '.heic', '.HEIC'}\n",
    "\n",
    "def get_image_files(patient_path):\n",
    "    \"\"\" Get all image files in a patient's folder \"\"\"\n",
    "    image_files = []\n",
    "    for file in os.listdir(patient_path):\n",
    "        if any(file.lower().endswith(ext) for ext in image_formats):\n",
    "            image_files.append(os.path.join(patient_path, file))\n",
    "    return image_files\n",
    "\n",
    "# Dictionary to store patient image counts\n",
    "patient_image_counts = {}\n",
    "\n",
    "# Iterate through conditions\n",
    "for condition in ['IRON DEFICIENCY ANEMIA', 'NON - IRON DEFICIENCY ANEMIA']:\n",
    "    condition_path = os.path.join(dataset_dir, condition)\n",
    "    if not os.path.exists(condition_path):\n",
    "        continue\n",
    "    \n",
    "    # Iterate through each patient folder\n",
    "    for patient_folder in os.listdir(condition_path):\n",
    "        patient_path = os.path.join(condition_path, patient_folder)\n",
    "        if os.path.isdir(patient_path):\n",
    "            img_files = get_image_files(patient_path)\n",
    "            patient_image_counts[patient_folder] = len(img_files)\n",
    "\n",
    "# Find patients with incorrect image counts\n",
    "incorrect_patients = {patient: count for patient, count in patient_image_counts.items() if count != 5}\n",
    "\n",
    "# Print the results\n",
    "if incorrect_patients:\n",
    "    print(\"Patients with incorrect image counts:\")\n",
    "    for patient, count in incorrect_patients.items():\n",
    "        print(f\"Patient: {patient}, Image Count: {count}\")\n",
    "else:\n",
    "    print(\"All patients have exactly 5 images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_labels[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm  # For progress bar\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define data transformations with additional augmentations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Check if GPU (CUDA) is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load a pre-trained ResNet model and modify the final layer for binary classification\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Freeze all layers initially\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the final fully connected layer\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),  # Dropout for regularization\n",
    "    nn.Linear(512, 2)\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Compute class weights (adjust labels accordingly)\n",
    "your_train_labels = all_train_labels\n",
    "class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=your_train_labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Define loss function with class weighting\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss, correct_predictions, total_predictions = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, correct_predictions, total_predictions = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Training loop with early stopping and layer unfreezing\n",
    "num_epochs = 25\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_patience = 5\n",
    "patience_counter = 0\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    # Unfreeze last ResNet block after 5 epochs\n",
    "    if epoch == 5:\n",
    "        for param in model.layer4.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"Unfreezing ResNet layer4 for fine-tuning\")\n",
    "\n",
    "    # Training phase\n",
    "    train_loss, train_accuracy = train(model, train_loader, criterion, optimizer, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Adjust learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_anemia_model.pth')  # Save best model\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break  # Stop training\n",
    "\n",
    "print(\"Training complete. Best model saved as 'best_anemia_model.pth'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'best_anemia_model.pth') \n",
    "torch.save(model.state_dict(), 'best_anemia_model.pt',_use_new_zipfile_serialization=True,pickle_protocol=4)\n",
    "torch.save(model.state_dict(), 'best_anemia_model.pth',_use_new_zipfile_serialization=True,pickle_protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set dark cool theme\n",
    "# plt.style.use('dark_background')\n",
    "\n",
    "# Create the visualizations directory if it doesn't exist\n",
    "os.makedirs(\"visualizations\", exist_ok=True)\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(\n",
    "    range(1, len(train_losses) + 1), \n",
    "    train_losses, \n",
    "    label='Train Loss', \n",
    "    marker='o',  # Add markers for data points\n",
    "    markersize=5,  # Adjust marker size\n",
    "    linestyle='-',  # Solid line\n",
    "    linewidth=2,  # Thicker line\n",
    "    color='cyan'  # Cool color for train loss\n",
    ")\n",
    "plt.plot(\n",
    "    range(1, len(val_losses) + 1), \n",
    "    val_losses, \n",
    "    label='Validation Loss', \n",
    "    marker='s',  # Add markers for data points\n",
    "    markersize=5,  # Adjust marker size\n",
    "    linestyle='--',  # Dashed line\n",
    "    linewidth=2,  # Thicker line\n",
    "    color='magenta'  # Cool color for validation loss\n",
    ")\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training and Validation Loss', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)  # Add grid for better readability\n",
    "plt.savefig(\n",
    "    os.path.join(\"visualizations\", \"training_validation_loss.png\"),\n",
    "    dpi=300,  # High resolution\n",
    "    bbox_inches='tight'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(\n",
    "    range(1, len(train_accuracies) + 1), \n",
    "    train_accuracies, \n",
    "    label='Train Accuracy', \n",
    "    marker='o',  # Add markers for data points\n",
    "    markersize=5,  # Adjust marker size\n",
    "    linestyle='-',  # Solid line\n",
    "    linewidth=2,  # Thicker line\n",
    "    color='lime'  # Cool color for train accuracy\n",
    ")\n",
    "plt.plot(\n",
    "    range(1, len(val_accuracies) + 1), \n",
    "    val_accuracies, \n",
    "    label='Validation Accuracy', \n",
    "    marker='s',  # Add markers for data points\n",
    "    markersize=5,  # Adjust marker size\n",
    "    linestyle='--',  # Dashed line\n",
    "    linewidth=2,  # Thicker line\n",
    "    color='orange'  # Cool color for validation accuracy\n",
    ")\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "plt.title('Training and Validation Accuracy', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)  # Add grid for better readability\n",
    "plt.savefig(\n",
    "    os.path.join(\"visualizations\", \"training_validation_accuracy.png\"),\n",
    "    dpi=300,  # High resolution\n",
    "    bbox_inches='tight'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def evaluate_model(model, test_loader, device, save_dir='visualizations'):\n",
    "    # Create visualizations directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            # Collecting labels and predictions\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(all_labels, all_predictions, output_dict=True)\n",
    "    \n",
    "    # Save classification report as markdown\n",
    "    report_path = os.path.join(save_dir, 'classification_report.md')\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"# Classification Report\\n\\n\")\n",
    "        f.write(\"## Metrics by Class\\n\\n\")\n",
    "        f.write(\"| Class | Precision | Recall | F1-Score | Support |\\n\")\n",
    "        f.write(\"|-------|-----------|--------|----------|----------|\\n\")\n",
    "        \n",
    "        for class_name, metrics in report.items():\n",
    "            if class_name not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "                f.write(f\"| {class_name} | {metrics['precision']:.2f} | {metrics['recall']:.2f} | {metrics['f1-score']:.2f} | {metrics['support']} |\\n\")\n",
    "        \n",
    "        f.write(\"\\n## Overall Metrics\\n\\n\")\n",
    "        f.write(f\"- **Accuracy**: {report['accuracy']:.2f}\\n\")\n",
    "        f.write(f\"- **Macro Avg Precision**: {report['macro avg']['precision']:.2f}\\n\")\n",
    "        f.write(f\"- **Macro Avg Recall**: {report['macro avg']['recall']:.2f}\\n\")\n",
    "        f.write(f\"- **Macro Avg F1-Score**: {report['macro avg']['f1-score']:.2f}\\n\")\n",
    "    \n",
    "    # Confusion Matrix with Dark Theme\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    # Dark theme heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', \n",
    "                cmap='crest', \n",
    "                cbar=False,\n",
    "                xticklabels=['Non-Anemic', 'Anemic'], \n",
    "                yticklabels=['Non-Anemic', 'Anemic'],\n",
    "                linewidths=0.5,\n",
    "                linecolor='gray',\n",
    "                square=True,\n",
    "                # Customize text color and background\n",
    "                annot_kws={'color': 'white', 'fontweight': 'bold'},\n",
    "                )\n",
    "    \n",
    "    # Customize plot aesthetics\n",
    "    plt.title('Confusion Matrix', color='black', fontsize=15, fontweight='bold')\n",
    "    plt.ylabel('Actual', color='black')\n",
    "    plt.xlabel('Predicted', color='black')\n",
    "    plt.xticks(color='black')\n",
    "    plt.yticks(color='black')\n",
    "\n",
    "    # Save the plot\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, 'confusion_matrix.png')\n",
    "    plt.savefig(save_path, edgecolor='none', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Classification report saved to {report_path}\")\n",
    "    print(f\"Confusion matrix saved to {save_path}\")\n",
    "\n",
    "# Usage remains the same\n",
    "evaluate_model(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing On Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the transformation (same as in training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to match model input size (224x224 for ResNet)\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization\n",
    "])\n",
    "\n",
    "\n",
    "# Load the trained model (make sure to load the weights)\n",
    "# Use ResNet50 instead of ResNet18 if it was used during training\n",
    "model = models.resnet50(pretrained=False)  # Don't use pre-trained weights, load our own\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)  # Modify the final layer for 2 output classes\n",
    "model.load_state_dict(torch.load('best_anemia_model.pth'))  # Load the saved weights\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Function to predict the class of an individual image\n",
    "def predict_image(image_path, model, transform, device):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path).convert('RGB')  # Ensure it's RGB format\n",
    "\n",
    "    # Apply transformations (resize, normalize, etc.)\n",
    "    image = transform(image)\n",
    "    \n",
    "    # Add batch dimension (model expects a batch, even if it's size 1)\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "\n",
    "    # Forward pass: get the model's prediction\n",
    "    with torch.no_grad():  # No need to calculate gradients during inference\n",
    "        outputs = model(image)\n",
    "        print(outputs)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the class with the highest probability\n",
    "\n",
    "    # Convert the prediction to label: 0 or 1 (Non-Anemic, Anemic)\n",
    "    label = 'Anemic' if predicted.item() == 1 else 'Non-Anemic'\n",
    "    return label\n",
    "\n",
    "# Test the prediction function\n",
    "image_path = r\"C:\\projs\\anemia_project\\real_data\\NON - IRON DEFICIENCY ANEMIA\\PATIENT 14\\palm.jpg\"  # Replace with your image path\n",
    "\n",
    "predicted_class = predict_image(image_path, model, transform, device)\n",
    "print(f\"Predicted Class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_17528\\212761145.py:84: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_anemia_model.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "Feature maps saved to visualizations\\Non_tongue_feature_maps.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "image_path = r\"C:\\projs\\anemia_project\\real_data\\NON - IRON DEFICIENCY ANEMIA\\PATIENT 14\\tongue.png\"  # Replace with your image path\n",
    "def visualize_feature_maps(image_path, model, transform, device, save_dir='visualizations'):\n",
    "    # Create visualizations directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Reset feature maps list\n",
    "    feature_maps.clear()\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Forward pass: Get the feature maps\n",
    "    with torch.no_grad():\n",
    "        model(image_tensor)\n",
    "\n",
    "    # Prepare visualization\n",
    "    num_feature_maps = feature_maps[0].shape[1]\n",
    "    print(num_feature_maps)\n",
    "    \n",
    "    # Create a dark-themed figure\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    grid_size = min(int(num_feature_maps**0.5) + 1, 8)\n",
    "    \n",
    "    for i in range(min(num_feature_maps, grid_size**2)):\n",
    "        plt.subplot(grid_size, grid_size, i + 1)\n",
    "        feature_map = feature_maps[0][0, i].cpu()\n",
    "        plt.imshow(feature_map, cmap='crest')\n",
    "        plt.title(f'FM {i+1}', color='black', fontsize=8)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Generate filename based on input image\n",
    "    base_filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    save_path = os.path.join(save_dir, f'Non_{base_filename}_feature_maps.png')\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(save_path,\n",
    "                edgecolor='none', \n",
    "                dpi=300,\n",
    "                bbox_inches='tight')\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Feature maps saved to {save_path}\")\n",
    "\n",
    "# Setup for feature map extraction\n",
    "def hook_fn(module, input, output):\n",
    "    feature_maps.append(output)\n",
    "\n",
    "# Global feature maps list\n",
    "feature_maps = []\n",
    "\n",
    "# Function to setup model for feature map extraction\n",
    "def setup_feature_map_extraction(model):\n",
    "    # Remove any existing hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    hooks.clear()\n",
    "    \n",
    "    # Register hook to the last convolutional block\n",
    "    hooks.append(\n",
    "        model.layer4[2].conv1.register_forward_hook(hook_fn)\n",
    "    )\n",
    "\n",
    "# Global hooks list\n",
    "hooks = []\n",
    "\n",
    "# Usage example (commented out)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet50(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model.load_state_dict(torch.load('best_anemia_model.pt'))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "setup_feature_map_extraction(model)\n",
    "visualize_feature_maps(image_path, model, transform, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_17528\\3973239298.py:84: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_anemia_model.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "Feature maps saved to visualizations\\Def_tongue_feature_maps.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "image_path = r\"C:\\projs\\anemia_project\\real_data\\IRON DEFICIENCY ANEMIA\\PATIENT 14\\tongue.jpg\"  # Replace with your image path\n",
    "def visualize_feature_maps(image_path, model, transform, device, save_dir='visualizations'):\n",
    "    # Create visualizations directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Reset feature maps list\n",
    "    feature_maps.clear()\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Forward pass: Get the feature maps\n",
    "    with torch.no_grad():\n",
    "        model(image_tensor)\n",
    "\n",
    "    # Prepare visualization\n",
    "    num_feature_maps = feature_maps[0].shape[1]\n",
    "    print(num_feature_maps)\n",
    "    \n",
    "    # Create a dark-themed figure\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    grid_size = min(int(num_feature_maps**0.5) + 1, 8)\n",
    "    \n",
    "    for i in range(min(num_feature_maps, grid_size**2)):\n",
    "        plt.subplot(grid_size, grid_size, i + 1)\n",
    "        feature_map = feature_maps[0][0, i].cpu()\n",
    "        plt.imshow(feature_map, cmap='crest')\n",
    "        plt.title(f'FM {i+1}', color='black', fontsize=8)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Generate filename based on input image\n",
    "    base_filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    save_path = os.path.join(save_dir, f'Def_{base_filename}_feature_maps.png')\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(save_path,\n",
    "                edgecolor='none', \n",
    "                dpi=300,\n",
    "                bbox_inches='tight')\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Feature maps saved to {save_path}\")\n",
    "\n",
    "# Setup for feature map extraction\n",
    "def hook_fn(module, input, output):\n",
    "    feature_maps.append(output)\n",
    "\n",
    "# Global feature maps list\n",
    "feature_maps = []\n",
    "\n",
    "# Function to setup model for feature map extraction\n",
    "def setup_feature_map_extraction(model):\n",
    "    # Remove any existing hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    hooks.clear()\n",
    "    \n",
    "    # Register hook to the last convolutional block\n",
    "    hooks.append(\n",
    "        model.layer4[2].conv1.register_forward_hook(hook_fn)\n",
    "    )\n",
    "\n",
    "# Global hooks list\n",
    "hooks = []\n",
    "\n",
    "# Usage example (commented out)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet50(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model.load_state_dict(torch.load('best_anemia_model.pt'))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "setup_feature_map_extraction(model)\n",
    "visualize_feature_maps(image_path, model, transform, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'anemic_real.pt',_use_new_zipfile_serialization=True,pickle_protocol=4)\n",
    "# torch.save(model.state_dict(), 'anemic_real.pth',_use_new_zipfile_serialization=True,pickle_protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudavenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
